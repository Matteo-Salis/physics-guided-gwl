{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray\n",
    "import rioxarray\n",
    "import fiona\n",
    "\n",
    "#import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from rasterio.enums import Resampling\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data.json: {'experiment_name': 'first_pinns_test_1D', 'wandb_dir': '/leonardo_scratch/fast/IscrC_DL4EO/github/water-pinns/src/runs', 'save_model_dir': '/leonardo_scratch/fast/IscrC_DL4EO/github/water-pinns/src/runs/continuous_1d', 'data_dir': '/leonardo_work/IscrC_DL4EO/trials/data/', 'wtd_csv_path': '/leonardo_work/IscrC_DL4EO/trials/data/dataset_wtd_roi.csv', 'weather_nc_path': '/leonardo_work/IscrC_DL4EO/trials/data/meteo_bucket_model_snowpack_ROI_1958_2023.nc', 'wtd_shp': '/leonardo_work/IscrC_DL4EO/trials/data/shapefile/underground_wtd_sensor_roi.shp', 'piedmont_shp': '/leonardo_work/IscrC_DL4EO/trials/data/shapefile/piemonte_admin_boundaries.shp', 'dtm_nc': '/leonardo_work/IscrC_DL4EO/trials/data/dtm_ROI.nc', 'weather_dtm': '/leonardo_work/IscrC_DL4EO/trials/data/dtm_ROI_arpa_weather.nc', 'timesteps': 180, 'test_split_p': 0.2, 'all_dataset': False, 'max_ds_elems': 1000, 'tensorboard': True, 'dataset': 'wtd_weather_2001_2023', 'input_channels': 10, 'input_height': 9, 'input_width': 12, 'shift_pixels': 0, 'augmentation': True, 'model': 'Discrete2DNN', 'start_epoch': 0, 'epochs': 10, 'batch_size': 128, 'optimizer': 'adam', 'lr': 0.0005, 'min_lr': 0, 'decay_rate': 0.99, 'decay_steps': 1, 'monitor': 'loss', 'iter_size': 1, 'patience': 10, 'use_cuda': True, 'cuda_device': 'cuda:0', 'num_workers': 1, 'print_every': 10, 'save_every': 1, 'seeds': [42]}\n"
     ]
    }
   ],
   "source": [
    "json_file = \"/leonardo_scratch/fast/IscrC_DL4EO/github/water-pinns/src/configs/discrete_2D_wtd/test_1D.json\"\n",
    "dict_files = {}\n",
    "with open(json_file) as f:\n",
    "    dict_files = json.load(f)\n",
    "    print(f\"Read data.json: {dict_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ContinuousDataset(Dataset):\n",
    "    \"\"\"Weather and WTD Dataset for the continuous case model\"\"\"\n",
    "\n",
    "    def __init__(self, dict_files, #meteo_nc_path, wtd_csv_path, wtd_stations_shp_path,\n",
    "                 fill_value = 0,\n",
    "                 transform = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dict_files (string): Path to the .nc file.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                    on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Attributes init\n",
    "        self.dict_files = dict_files\n",
    "        self.timesteps = self.dict_files[\"timesteps\"]\n",
    "\n",
    "        # Meteorological data loading \n",
    "        self.loading_weather()\n",
    "        \n",
    "        # Digital Terrain Model data loading\n",
    "        self.loading_dtm()\n",
    "        \n",
    "        # Water Table Depth data loading \n",
    "        self.loading_point_wtd(fill_value = fill_value)\n",
    "\n",
    "        # Transform       \n",
    "        self.transform = transform\n",
    "        \n",
    "    def loading_dtm(self):\n",
    "        self.dtm_roi = rioxarray.open_rasterio(self.dict_files[\"dtm_nc\"],\n",
    "                                               engine='fiona')\n",
    "        self.dtm_roi = self.dtm_roi.rio.write_crs(\"epsg:4326\")\n",
    "        \n",
    "            \n",
    "    def loading_weather(self):\n",
    "        self.weather_xr = xarray.open_dataset(self.dict_files[\"weather_nc_path\"])\n",
    "        self.weather_xr = self.weather_xr.rio.write_crs(\"epsg:4326\")\n",
    "        \n",
    "        # Compute coord matrix\n",
    "        lat_matrix = np.vstack([self.weather_xr.lat.values for i in range(len(self.weather_xr.lon.values))]).transpose()\n",
    "        lon_matrix = np.vstack([self.weather_xr.lon.values for i in range(len(self.weather_xr.lat.values))])\n",
    "        \n",
    "        self.weather_coords = np.stack([lat_matrix,lon_matrix], axis = -1)\n",
    "        \n",
    "        self.weather_dtm = rioxarray.open_rasterio(self.dict_files[\"weather_dtm\"],\n",
    "                                               engine='fiona')\n",
    "        \n",
    "        self.weather_dtm = self.weather_dtm.values\n",
    "        self.weather_dtm = np.moveaxis(self.weather_dtm, 0,-1)\n",
    "\n",
    "    def loading_point_wtd(self, fill_value = 0):\n",
    "        \n",
    "        # Water Table Depth data loading\n",
    "        self.wtd_df = pd.read_csv(self.dict_files[\"wtd_csv_path\"], \n",
    "                                    dtype= {\"sensor_id\": \"str\"})\n",
    "        self.wtd_df = self.wtd_df.astype({\"date\":'datetime64[ns]'})\n",
    "\n",
    "        # Water Table Depth Sensors shapefile loading: \n",
    "        self.wtd_names = gpd.read_file(self.dict_files[\"wtd_shp\"],\n",
    "                                             engine='fiona')\n",
    "        self.wtd_names = self.wtd_names.to_crs('epsg:4326')\n",
    "\n",
    "        # Define attributes about dates and coordinates\n",
    "        self.dates = self.wtd_df[\"date\"].unique()\n",
    "        self.sensor_id_list = self.wtd_df[\"sensor_id\"].unique()\n",
    "        \n",
    "        \n",
    "        ### Merge csv and shp into a joint spatio temporal representation\n",
    "        sensor_coord_x_list = []\n",
    "        sensor_coord_y_list = []\n",
    "\n",
    "        # Retrieve coordinates from id codes\n",
    "        for sensor in self.sensor_id_list:\n",
    "            coord_x = self.wtd_names.loc[self.wtd_names[\"sensor_id\"] == sensor].geometry.x.values[0]\n",
    "            coord_y = self.wtd_names.loc[self.wtd_names[\"sensor_id\"] == sensor].geometry.y.values[0]\n",
    "            sensor_coord_x_list.append(coord_x)\n",
    "            sensor_coord_y_list.append(coord_y)\n",
    "\n",
    "        # Buil a dictionary of coordinates and id codes\n",
    "        from_id_to_coord_x_dict = {self.sensor_id_list[i]: sensor_coord_x_list[i] for i in range(len(sensor_coord_x_list))}\n",
    "        from_id_to_coord_y_dict = {self.sensor_id_list[i]: sensor_coord_y_list[i] for i in range(len(sensor_coord_y_list))}\n",
    "\n",
    "        # Map id codes to coordinates for all rows in the original ds\n",
    "        queries = list(self.wtd_df[\"sensor_id\"].values)\n",
    "        coordinates_x = itemgetter(*queries)(from_id_to_coord_x_dict)\n",
    "        coordinates_y = itemgetter(*queries)(from_id_to_coord_y_dict)\n",
    "\n",
    "        # insert new columns containing coordinates\n",
    "        self.wtd_df[\"x\"] = coordinates_x\n",
    "        self.wtd_df[\"y\"] = coordinates_y\n",
    "        \n",
    "        self.wtd_df = self.wtd_df.set_index([\"date\",\"y\",\"x\"])\n",
    "        \n",
    "        # Subset wtd data truncating the last `timestep` instances\n",
    "        last_date = self.dates.max() - np.timedelta64(self.timesteps, 'D')\n",
    "        self.input_dates = self.dates[self.dates <= last_date]\n",
    "        \n",
    "        # Create nan-mask\n",
    "        self.wtd_df[\"nan_mask\"] = 1*~self.wtd_df[\"wtd\"].isna()\n",
    "        self.wtd_df[\"wtd\"] = self.wtd_df[\"wtd\"].fillna(fill_value)\n",
    "        \n",
    "    def __len__(self):\n",
    "        data = self.wtd_df.loc[pd.IndexSlice[self.wtd_df.index.get_level_values(0) <= self.input_dates.max(),\n",
    "                                                       :,\n",
    "                                                       :]]\n",
    "        return len(data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if idx < 0:\n",
    "            idx = self.__len__() + idx\n",
    "        \n",
    "        # Retrieve date and coords for idx instance\n",
    "        start_date = self.wtd_df.iloc[idx, :].dropna().name[0]\n",
    "        sample_lat = self.wtd_df.iloc[idx, :].dropna().name[1]\n",
    "        sample_lon = self.wtd_df.iloc[idx, :].dropna().name[2]\n",
    "        sample_dtm = self.dtm_roi.sel(x = sample_lon,\n",
    "                                      y = sample_lat,\n",
    "                                      method = \"nearest\").values  \n",
    "        \n",
    "        end_date = start_date + np.timedelta64(self.timesteps, \"D\")\n",
    "        \n",
    "        # print(\"start date: \", str(start_date))\n",
    "        # print(\"end date: \", str(end_date))\n",
    "        \n",
    "        # Initial state WTD (t0) data\n",
    "        wtd_t0 = self.wtd_df[[\"wtd\", \"nan_mask\"]].loc[self.wtd_df.index.get_level_values(0) == start_date].dropna()\n",
    "        wtd_t0_values = wtd_t0[\"wtd\"].values\n",
    "        #wtd_t0_mask = wtd_t0[\"nan_mask\"].values\n",
    "        wtd_t0_lat = wtd_t0.index.get_level_values(1).values\n",
    "        wtd_t0_lon = wtd_t0.index.get_level_values(2).values\n",
    "        wtd_t0_dtm = np.array([self.dtm_roi.sel(x = wtd_t0_lon[sensor],\n",
    "                                                y = wtd_t0_lat[sensor],\n",
    "                                                method = \"nearest\") for sensor in range(len(wtd_t0_lat))]).squeeze()\n",
    "        \n",
    "        #wtd_t0_mask = 1*~np.isnan(wtd_t0_values)\n",
    "        X = [torch.from_numpy(wtd_t0_lat).to(torch.float32),\n",
    "             torch.from_numpy(wtd_t0_lon).to(torch.float32),\n",
    "             torch.from_numpy(wtd_t0_dtm).to(torch.float32),\n",
    "             torch.from_numpy(wtd_t0_values).to(torch.float32),\n",
    "             #torch.from_numpy(wtd_t0_mask).to(torch.float32)\n",
    "             ]\n",
    "        X = torch.stack(X, dim = -1)\n",
    "        \n",
    "        Z = [torch.tensor(sample_lat).reshape(1).to(torch.float32),\n",
    "             torch.tensor(sample_lon).reshape(1).to(torch.float32),\n",
    "             torch.tensor(sample_dtm).reshape(1).to(torch.float32)]\n",
    "        \n",
    "        Z = torch.stack(Z, dim = -1).squeeze()\n",
    "        \n",
    "        # Retrieve weather data\n",
    "        weather_video = self.weather_xr.sel(time = slice(start_date + np.timedelta64(1, \"D\"),\n",
    "                                                    end_date)) #slice include extremes\n",
    "        weather_video = weather_video.to_array().values\n",
    "        W = torch.from_numpy(weather_video).to(torch.float32)\n",
    "        \n",
    "        # Retrieve wtd values from t0+1 to T for the idx instance sensor\n",
    "        wtd_t1_T = self.wtd_df[[\"wtd\", \"nan_mask\"]].loc[(self.wtd_df.index.get_level_values(0) > start_date) &\n",
    "                                          (self.wtd_df.index.get_level_values(0) <= end_date)  & \n",
    "                                          (self.wtd_df.index.get_level_values(1) == sample_lat)&\n",
    "                                          (self.wtd_df.index.get_level_values(2) == sample_lon)]\n",
    "        \n",
    "        wtd_t1_T_values =  wtd_t1_T[\"wtd\"].values\n",
    "        wtd_t1_T_mask =  wtd_t1_T[\"nan_mask\"].values        \n",
    "        \n",
    "        Y = [torch.from_numpy(wtd_t1_T_values).to(torch.float32),\n",
    "             torch.from_numpy(wtd_t1_T_mask).to(torch.float32)]\n",
    "        \n",
    "        Y = torch.stack(Y, dim = -1)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return [X, Z, W, Y]\n",
    "    \n",
    "    def get_weather_dtm(self):\n",
    "        return torch.from_numpy(self.weather_dtm).to(torch.float32)\n",
    "        \n",
    "    def get_weather_coords(self):\n",
    "        return torch.from_numpy(self.weather_coords).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ContinuousDataset(dict_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset: 254820\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the dataset: {ds.__len__()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0][0][:,:3].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "class Continuous1DNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 timestep = 180,\n",
    "                 cb_fc_layer = 5,\n",
    "                 cb_fc_neurons = 16,\n",
    "                 conv_filters = 16,\n",
    "                 lstm_layer = 3,\n",
    "                 lstm_input_units = 16,\n",
    "                 lstm_units = 32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.timestep = timestep\n",
    "        self.lstm_layer = lstm_layer\n",
    "        self.lstm_input_units = lstm_input_units\n",
    "        self.lstm_units = lstm_units\n",
    "        self.cb_fc_layer = cb_fc_layer\n",
    "        self.cb_fc_neurons = cb_fc_neurons\n",
    "        self.conv_filters = conv_filters\n",
    "        \n",
    "        self.wgamma = nn.Sigmoid()\n",
    "        # Fully connected\n",
    "        cb_fc = []\n",
    "        cb_fc.append(nn.Linear(4, self.cb_fc_neurons))\n",
    "        cb_fc.append(nn.ReLU())\n",
    "        for l in range(self.cb_fc_layer - 2):\n",
    "            cb_fc.append(nn.Linear(self.cb_fc_neurons, self.cb_fc_neurons))\n",
    "            cb_fc.append(nn.ReLU())\n",
    "        \n",
    "        cb_fc.append(nn.Linear(self.cb_fc_neurons, self.lstm_units))\n",
    "        cb_fc.append(nn.ReLU())\n",
    "        self.cb_fc = nn.Sequential(*cb_fc)\n",
    "        \n",
    "        # Weather block\n",
    "        self.weather_wgamma = nn.Sigmoid()\n",
    "        \n",
    "        conv3d_stack=[]\n",
    "        conv3d_stack.append(nn.Conv3d(10, self.conv_filters, (1,2,2))) # Conv input (N, C, D, H, W) - kernel 3d (D, H, W)\n",
    "        conv3d_stack.append(nn.BatchNorm3d(self.conv_filters))\n",
    "        conv3d_stack.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(4):\n",
    "            conv3d_stack.append(nn.Conv3d(self.conv_filters, self.conv_filters, (1,2,2)))\n",
    "            conv3d_stack.append(nn.BatchNorm3d(self.conv_filters))\n",
    "            conv3d_stack.append(nn.ReLU())\n",
    "            \n",
    "        conv3d_stack.append(nn.AdaptiveAvgPool3d((None,4,4)))\n",
    "        conv3d_stack.append(nn.Conv3d(self.conv_filters, self.conv_filters, (1,2,2)))\n",
    "        conv3d_stack.append(nn.BatchNorm3d(self.conv_filters))\n",
    "        conv3d_stack.append(nn.ReLU())\n",
    "        conv3d_stack.append(nn.Conv3d(self.conv_filters, self.conv_filters, (1,2,2)))\n",
    "        conv3d_stack.append(nn.BatchNorm3d(self.conv_filters))\n",
    "        conv3d_stack.append(nn.ReLU())\n",
    "        conv3d_stack.append(nn.Conv3d(self.conv_filters, self.lstm_input_units, (1,2,2)))\n",
    "        conv3d_stack.append(nn.BatchNorm3d(self.lstm_input_units))\n",
    "        conv3d_stack.append(nn.ReLU())\n",
    "        self.conv3d_stack = nn.Sequential(*conv3d_stack)\n",
    "            \n",
    "        # Joint sequental block\n",
    "        self.lstm_1 = nn.LSTM(self.lstm_input_units, self.lstm_units,\n",
    "                              batch_first=True,\n",
    "                              num_layers=self.lstm_layer) # Batch first input (N,L,H)\n",
    "        \n",
    "        fc = []\n",
    "        fc.append(nn.Linear(self.lstm_units, 8))\n",
    "        fc.append(nn.ReLU())\n",
    "        fc.append(nn.Linear(8, 1))\n",
    "        self.fc = nn.Sequential(*fc)\n",
    "\n",
    "\n",
    "    def forward(self, x, z, w):\n",
    "        \"\"\"\n",
    "        input : x (31, 5); z (1, 3); w[0] (10, 180, 9, 12); w[1] (9, 12, 3)\n",
    "        return \n",
    "            lstm_out (array): lstm_out = [S_we, M, P_r, Es, K_s, K_r]\n",
    "        x: tensor of shape (L,Hin) if minibatches itaration (L,N,Hin) when batch_first=False (default)\n",
    "        \"\"\"\n",
    "        \n",
    "        # #x (31, 5); z (3); w[0] (10, 180, 9, 12); w[1] (9, 12, 3); y (180, 2)\n",
    "        # [wtd_t0_lat, wtd_t0_lon,\n",
    "        #  wtd_t0_dtm, wtd_t0_values,\n",
    "        #  wtd_t0_mask] = x\n",
    "        \n",
    "        # [sample_lat, sample_lon, sample_dtm] = z\n",
    "        \n",
    "        # Conditioning block\n",
    "        \n",
    "        \n",
    "        wtd_sim = torch.matmul(x[:,:,:3], z.unsqueeze(-1))\n",
    "        wtd_sim = self.wgamma(wtd_sim)\n",
    "        print(\"sim\", wtd_sim.shape)\n",
    "        \n",
    "        wtd0 = torch.sum(x[:,:,:-1] * wtd_sim, dim = (1,2))/torch.sum(wtd_sim, dim = (1,2))\n",
    "        print(\"wtd0\", wtd0.shape)\n",
    "        print(\"z\", z.shape)\n",
    "        wtd0 = torch.cat([z, wtd0.unsqueeze(-1)], dim = -1)\n",
    "        print(\"wtd0\", wtd0.shape)\n",
    "        \n",
    "        wtd0 = self.cb_fc(wtd0)\n",
    "        print(\"wtd0 emb\", wtd0.shape)\n",
    "        \n",
    "        # Weather block\n",
    "        ## w[0] (10, 180, 9, 12); w[1] (9, 12, 3)\n",
    "        weather_sim = w[1] * z[:,None,None,:].expand(-1, w[1].shape[1], w[1].shape[2], -1)\n",
    "        weather_sim = torch.sum(weather_sim, dim = -1)\n",
    "        weather_sim = self.weather_wgamma(weather_sim)\n",
    "        weather_sim = weather_sim[:, None, None, : ,:].expand(-1, w[0].shape[1], w[0].shape[2], -1, -1 )\n",
    "        \n",
    "        weather = w[0] * weather_sim\n",
    "        \n",
    "        wb_td3dconv = self.conv3d_stack(weather)\n",
    "        \n",
    "        wb_td3dconv = wb_td3dconv.squeeze()\n",
    "        wb_td3dconv = torch.moveaxis(wb_td3dconv, 1, -1)\n",
    "        \n",
    "        # Sequential block\n",
    "        wtd0 = wtd0.unsqueeze(1).expand([-1,self.lstm_layer,-1])\n",
    "        wtd0 = torch.movedim(wtd0, 0, 1)\n",
    "        \n",
    "        wtd_series = self.lstm_1(wb_td3dconv,\n",
    "                                 (wtd0.contiguous(),\n",
    "                                  wtd0.contiguous())) #input  [input, (h_0, c_0)] - h and c (D∗num_layers,N,H)\n",
    "        \n",
    "        wtd_series = self.fc(wtd_series[0])\n",
    "        \n",
    "        return wtd_series.squeeze()\n",
    "\n",
    "model = Continuous1DNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters:  33201\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of trainable parameters: \" ,sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing size: 800, Test size: 200\n"
     ]
    }
   ],
   "source": [
    "batch_size = dict_files[\"batch_size\"]\n",
    "max_epochs = dict_files[\"epochs\"]\n",
    "\n",
    "test_split_p = dict_files[\"test_split_p\"]\n",
    "train_split_p = 1 - test_split_p\n",
    "\n",
    "max_ds_elems = ds.__len__()\n",
    "if not dict_files[\"all_dataset\"]:\n",
    "    max_ds_elems = dict_files[\"max_ds_elems\"]\n",
    "\n",
    "train_idx = int(max_ds_elems*train_split_p)\n",
    "test_idx = int(max_ds_elems*test_split_p)\n",
    "\n",
    "print(f\"Traing size: {train_idx}, Test size: {test_idx}\")\n",
    "\n",
    "train_idxs, test_idxs = np.arange(train_idx), np.arange(train_idx, train_idx + test_idx)\n",
    "\n",
    "train_sampler = SequentialSampler(train_idxs)\n",
    "test_sampler = SequentialSampler(test_idxs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=ds,\n",
    "                                            batch_size=batch_size,\n",
    "                                            sampler=train_sampler)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=ds,\n",
    "                                            batch_size=batch_size,\n",
    "                                            sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(x, y, y_hat, save_dir = None):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.suptitle(\"Loss vs iterations\")\n",
    "    ax.plot(x, y_hat, label = \"predicted\")\n",
    "    ax.plot(x, y, label = \"true\")\n",
    "    ax.legend()\n",
    "    if save_dir:\n",
    "        plt.savefig(f\"{save_dir}.png\", bbox_inches = 'tight') #dpi = 400, transparent = True\n",
    "    else:\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse(y_hat, y, mask):\n",
    "    # y_hat = y_hat.to(device)\n",
    "    # y = y.to(device)\n",
    "    # mask = mask.to(device)\n",
    "    return torch.sum(((y_hat-y)*mask)**2.0)  / torch.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7hvbzgqq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /leonardo_scratch/fast/IscrC_DL4EO/github/water-pinns/src/runs/wandb/offline-run-20241221_125337-7hvbzgqq<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/leonardo_scratch/fast/IscrC_DL4EO/github/water-pinns/src/runs/wandb/offline-run-20241221_125337-7hvbzgqq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7hvbzgqq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "    entity=\"gsartor-unito\",\n",
    "    project=dict_files[\"experiment_name\"],\n",
    "    dir =dict_files[\"wandb_dir\"],\n",
    "    config=dict_files,\n",
    "    mode=\"offline\",\n",
    ")\n",
    "\n",
    "# Magic\n",
    "wandb.watch(model, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem allocated in MB:  0.0\n",
      "############### Training epoch 0 ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/7 [00:05<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim torch.Size([128, 31, 1])\n",
      "wtd0 torch.Size([128])\n",
      "z torch.Size([128, 3])\n",
      "wtd0 torch.Size([128, 4])\n",
      "wtd0 emb torch.Size([128, 32])\n",
      "Train loss: 469.8125305175781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  14%|█▍        | 1/7 [00:20<01:24, 14.12s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim torch.Size([128, 31, 1])\n",
      "wtd0 torch.Size([128])\n",
      "z torch.Size([128, 3])\n",
      "wtd0 torch.Size([128, 4])\n",
      "wtd0 emb torch.Size([128, 32])\n",
      "Train loss: 439.1970520019531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  29%|██▊       | 2/7 [00:23<00:55, 11.06s/batch]"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "#save_dir = \"/leonardo_scratch/fast/IscrC_DL4EO/github/water-pinns/src/runs/continuous_1d\"\n",
    "\n",
    "weather_coords = ds.get_weather_coords()\n",
    "weather_dtm = ds.get_weather_dtm()\n",
    "weather_coords = torch.cat([weather_coords, weather_dtm], dim = -1)\n",
    "weather_coords = weather_coords.unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "\n",
    "print('mem allocated in MB: ', torch.cuda.memory_allocated() / 1024**2)\n",
    "#print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "for i in range(max_epochs):\n",
    "    \n",
    "    model.train(True)\n",
    "    start_time = time.time()\n",
    "    print(f\"############### Training epoch {i} ###############\")\n",
    "    \n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            for batch_idx, (x, z, w_values, y) in enumerate(tepoch):\n",
    "                tepoch.set_description(f\"Epoch {i}\")\n",
    "                \n",
    "                x = x.to(device)\n",
    "                z = z.to(device)\n",
    "                w = [w_values.to(device), weather_coords.to(device)]\n",
    "                y = y.to(device)\n",
    "                #print('Batch mem allocated in MB: ', torch.cuda.memory_allocated() / 1024**2)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                y_hat = model(x, z, w)\n",
    "                #print('After predict mem allocated in MB: ', torch.cuda.memory_allocated() / 1024**2)\n",
    "                loss = masked_mse(y_hat,\n",
    "                                  y[:,:,0],\n",
    "                                  y[:,:,1])\n",
    "                \n",
    "                print(f\"Train loss: {loss}\")\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                metrics = {\n",
    "                    \"train_loss\" : loss\n",
    "                }\n",
    "                wandb.log(metrics)              \n",
    "                \n",
    "    end_time = time.time()\n",
    "    exec_time = end_time-start_time\n",
    "\n",
    "    wandb.log({\"tr_epoch_exec_t\" : exec_time})\n",
    "\n",
    "    model_name = 'model_{}_{}.pt'.format(timestamp, i)    \n",
    "    model_dir = dict_files[\"save_model_dir\"]\n",
    "    torch.save(model.state_dict(), f\"{model_dir}/{model_name}\") \n",
    "\n",
    "    print(f\"############### Test epoch {i} ###############\")\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "                for batch_idx, (x, z, w_values, y) in enumerate(tepoch):\n",
    "                    tepoch.set_description(f\"Epoch {i}\")\n",
    "\n",
    "                    x = x.to(device)\n",
    "                    z = z.to(device)\n",
    "                    w = [w_values.to(device), weather_coords.to(device)]\n",
    "                    y = y.to(device)\n",
    "                    # print('Batch mem allocated in MB: ', torch.cuda.memory_allocated() / 1024**2)\n",
    "\n",
    "                    y_hat = model(x, z, w)\n",
    "                    # print('After predict mem allocated in MB: ', torch.cuda.memory_allocated() / 1024**2)\n",
    "\n",
    "                    loss = masked_mse(y_hat,\n",
    "                                  y[:,:,0],\n",
    "                                  y[:,:,1])\n",
    "                    print(f\"Test loss: {loss}\")\n",
    "\n",
    "                    metrics = {\n",
    "                        \"test_loss\" : loss\n",
    "                    }\n",
    "\n",
    "                    wandb.log(metrics)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    exec_time = end_time-start_time\n",
    "    wandb.log({\"test_epoch_exec_t\" : exec_time})\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "print(f\"Execution time: {end_time-start_time}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
