{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/leonardo_scratch/fast/IscrC_DL4EO/github/water-pinns/src')  # Provide the new path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "Other supported backends: tensorflow.compat.v1, tensorflow, jax, paddle.\n",
      "paddle supports more examples now and is recommended.\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray\n",
    "import rioxarray\n",
    "import fiona\n",
    "\n",
    "#import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from rasterio.enums import Resampling\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torchview\n",
    "from torchview import draw_graph\n",
    "\n",
    "from utils.prediction_plot_1d import *\n",
    "\n",
    "from models.load_models_1d import *\n",
    "from dataloaders.load_1d_meteo_wtd import ContinuousDataset\n",
    "from subprocess import Popen\n",
    "\n",
    "import deepxde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "prova = torch.tensor([0.,1.,2.], requires_grad=False)\n",
    "print(prova)\n",
    "print(prova.requires_grad)\n",
    "prova.requires_grad_()\n",
    "print(prova.requires_grad)\n",
    "prova.requires_grad_(False)\n",
    "print(prova.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) real function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[0., 1., 2., 3., 4., 5.],\n",
      "        [0., 1., 2., 3., 4., 6.]], requires_grad=True)\n",
      "torch.Size([2, 6])\n",
      "Y: tensor([[ 0.,  1.,  4.,  9., 16., 25.],\n",
      "        [ 0.,  1.,  4.,  9., 16., 36.]], grad_fn=<PowBackward0>)\n",
      "torch.Size([2, 6])\n",
      "\n",
      "Gradients: \n",
      " (tensor([[ 0.,  2.,  4.,  6.,  8., 10.],\n",
      "        [ 0.,  2.,  4.,  6.,  8., 12.]]),)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0.,1.,2.,3.,4.,5.],\n",
    "                  [0.,1.,2.,3.,4.,6.]], requires_grad=True)\n",
    "print(\"X:\", x)\n",
    "print(x.shape)\n",
    "\n",
    "y = x**2\n",
    "\n",
    "print(\"Y:\", y)\n",
    "print(y.shape)\n",
    "\n",
    "print(\"\\nGradients: \\n\", torch.autograd.grad(y, x, grad_outputs= torch.ones_like(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) function of vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.autograd is an engine for computing vector-Jacobian product. That is, given any vector v compute: J.T @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[ 1.0050,  2.7000,  3.0000],\n",
      "        [ 1.0800,  2.5000,  4.0000],\n",
      "        [ 0.9000,  0.4500,  0.0000],\n",
      "        [-0.9000,  0.5000,  0.0000]], requires_grad=True)\n",
      "torch.Size([4, 3])\n",
      "Y: tensor([[  6.7050,  -6.7050,  16.4100,  -7.3050],\n",
      "        [  7.5800,  -7.5800,  23.1600, -10.5800],\n",
      "        [  1.3500,  -1.3500,   2.7000,  -0.4500],\n",
      "        [ -0.4000,   0.4000,  -0.8000,   1.4000]], grad_fn=<CopySlices>)\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.005,2.7,3.],\n",
    "                  [1.080,2.5,4.],\n",
    "                  [0.90,0.45,0.],\n",
    "                  [-0.90,0.5,0.]], requires_grad=True)\n",
    "print(\"X:\", x)\n",
    "print(x.shape)\n",
    "\n",
    "y = torch.ones(4,4)\n",
    "y[:, 0] = x[:, 0] + x[:, 1] + x[:, 2] # gradients = 1 + 1 + 1 \n",
    "y[:, 1] = - x[:, 0] - x[:, 1] - x[:, 2] # gradients = - 1 - 1 - 1\n",
    "y[:, 2] = 2*x[:, 0] + 2*x[:, 1] + x[:, 2]**2 # gradients = 2 + 2 + 2x\n",
    "y[:, 3] = - x[:, 0] + x[:, 1] - 3*x[:, 2]\n",
    "\n",
    "print(\"Y:\", y)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "# grad_mask = torch.tensor([[[1,0,0,0], # lavora sulle batch - 1 vettore riga per ogni istanza\n",
    "#                           [1,0,0,0]],\n",
    "                          \n",
    "#                           [[0,1,0,0], # itero sul gradiente che voglio estrarre\n",
    "#                           [0,1,0,0]],\n",
    "                          \n",
    "#                           [[0,0,1,0], # itero sul gradiente che voglio estrarre\n",
    "#                           [0,0,1,0]],\n",
    "                          \n",
    "#                           [[0,0,0,1], # itero sul gradiente che voglio estrarre\n",
    "#                           [0,0,0,1]]])\n",
    "\n",
    "grad_mask = torch.eye(y.shape[-1])\n",
    "grad_mask = grad_mask[:,None,:].expand(-1, y.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_bk_loop(y, x, x_features = [0,1,2]):\n",
    "    gradients_bk = []\n",
    "    for b in range(4):\n",
    "        b_grad = []\n",
    "        for i in range(4):\n",
    "            y[b,i].backward(retain_graph=True)\n",
    "            b_grad.append(x.grad[b,x_features].clone())\n",
    "            x.grad.zero_() \n",
    "        gradients_bk.append(b_grad)\n",
    "    \n",
    "    return gradients_bk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([1., 1., 1.]),\n",
       "  tensor([-1., -1., -1.]),\n",
       "  tensor([2., 2., 6.]),\n",
       "  tensor([-1.,  1., -3.])],\n",
       " [tensor([1., 1., 1.]),\n",
       "  tensor([-1., -1., -1.]),\n",
       "  tensor([2., 2., 8.]),\n",
       "  tensor([-1.,  1., -3.])],\n",
       " [tensor([1., 1., 1.]),\n",
       "  tensor([-1., -1., -1.]),\n",
       "  tensor([2., 2., 0.]),\n",
       "  tensor([-1.,  1., -3.])],\n",
       " [tensor([1., 1., 1.]),\n",
       "  tensor([-1., -1., -1.]),\n",
       "  tensor([2., 2., 0.]),\n",
       "  tensor([-1.,  1., -3.])]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_bk_loop(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradien backprop loop: \n",
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1.])\n",
      "\n",
      "\n",
      "tensor([-1., -1., -1.])\n",
      "tensor([-1., -1., -1.])\n",
      "tensor([-1., -1., -1.])\n",
      "tensor([-1., -1., -1.])\n",
      "\n",
      "\n",
      "tensor([2., 2., 6.])\n",
      "tensor([2., 2., 8.])\n",
      "tensor([2., 2., 0.])\n",
      "tensor([2., 2., 0.])\n",
      "\n",
      "\n",
      "tensor([-1.,  1., -3.])\n",
      "tensor([-1.,  1., -3.])\n",
      "tensor([-1.,  1., -3.])\n",
      "tensor([-1.,  1., -3.])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradien backprop loop: \")\n",
    "for i in range(4):\n",
    "    print(grad_bk_loop(y, x)[0][i])\n",
    "    print(grad_bk_loop(y, x)[1][i])\n",
    "    print(grad_bk_loop(y, x)[2][i])\n",
    "    print(grad_bk_loop(y, x)[3][i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradients - Autograd: \n",
      " (tensor([[[ 1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.],\n",
      "         [ 1.,  1.,  1.]],\n",
      "\n",
      "        [[-1., -1., -1.],\n",
      "         [-1., -1., -1.],\n",
      "         [-1., -1., -1.],\n",
      "         [-1., -1., -1.]],\n",
      "\n",
      "        [[ 2.,  2.,  6.],\n",
      "         [ 2.,  2.,  8.],\n",
      "         [ 2.,  2.,  0.],\n",
      "         [ 2.,  2.,  0.]],\n",
      "\n",
      "        [[-1.,  1., -3.],\n",
      "         [-1.,  1., -3.],\n",
      "         [-1.,  1., -3.],\n",
      "         [-1.,  1., -3.]]]),)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 3])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients = torch.autograd.grad(y, x, grad_outputs= grad_mask, is_grads_batched=True, retain_graph=True)\n",
    "\n",
    "# Autograd: Compute and return the sum of gradients of outputs with respect to the inputs.\n",
    "print(\"\\nGradients - Autograd: \\n\", gradients) # grad_outputs should be a sequence of length matching output containing the “vector” in vector-Jacobian product (v^T@J)\n",
    "gradients[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set the default automatic differentiation to reverse mode.\n",
      "\n",
      "Gradients - Autograd: \n",
      " tensor([[-1.,  1., -3.],\n",
      "        [-1.,  1., -3.],\n",
      "        [-1.,  1., -3.],\n",
      "        [-1.,  1., -3.]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepxde.config.set_default_autodiff(\"reverse\")\n",
    "gradients_dxde = deepxde.gradients.gradients.jacobian(y, x, i = 3)\n",
    "# Autograd: Compute and return the sum of gradients of outputs with respect to the inputs.\n",
    "print(\"\\nGradients - Autograd: \\n\", gradients_dxde) # grad_outputs should be a sequence of length matching output containing the “vector” in vector-Jacobian product (v^T@J)\n",
    "gradients_dxde[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) One dimensional input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[ 1.0050],\n",
      "        [ 1.0800],\n",
      "        [ 0.9000],\n",
      "        [-0.9000]], requires_grad=True)\n",
      "torch.Size([4, 1])\n",
      "Y: tensor([[ 3.0150, -3.0150,  5.0300, -3.0150],\n",
      "        [ 3.2400, -3.2400,  5.4864, -3.2400],\n",
      "        [ 2.7000, -2.7000,  4.4100, -2.7000],\n",
      "        [-2.7000,  2.7000, -2.7900,  2.7000]], grad_fn=<CopySlices>)\n",
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.005],\n",
    "                  [1.080],\n",
    "                  [0.90],\n",
    "                  [-0.90]], requires_grad=True)\n",
    "print(\"X:\", x)\n",
    "print(x.shape)\n",
    "\n",
    "def y_fun(x):\n",
    "    y = torch.ones(4,4)\n",
    "    y[:, 0] = x[:, 0] + x[:, 0] + x[:, 0] # gradients = 1 + 1 + 1 \n",
    "    y[:, 1] = - x[:, 0] - x[:, 0] - x[:, 0] # gradients = - 1 - 1 - 1\n",
    "    y[:, 2] = 2*x[:, 0] + 2*x[:, 0] + x[:, 0]**2 # gradients = 2 + 2 + 2x\n",
    "    y[:, 3] = - x[:, 0] + x[:, 0] - 3*x[:, 0]\n",
    "    return y\n",
    "\n",
    "y = y_fun(x)\n",
    "print(\"Y:\", y)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "# grad_mask = torch.tensor([[[1,0,0,0], # lavora sulle batch - 1 vettore riga per ogni istanza\n",
    "#                           [1,0,0,0]],\n",
    "                          \n",
    "#                           [[0,1,0,0], # itero sul gradiente che voglio estrarre\n",
    "#                           [0,1,0,0]],\n",
    "                          \n",
    "#                           [[0,0,1,0], # itero sul gradiente che voglio estrarre\n",
    "#                           [0,0,1,0]],\n",
    "                          \n",
    "#                           [[0,0,0,1], # itero sul gradiente che voglio estrarre\n",
    "#                           [0,0,0,1]]])\n",
    "\n",
    "grad_mask = torch.eye(y.shape[-1])\n",
    "grad_mask = grad_mask[:,None,:].expand(-1, y.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradients - Autograd: \n",
      " (tensor([[[ 3.0000],\n",
      "         [ 3.0000],\n",
      "         [ 3.0000],\n",
      "         [ 3.0000]],\n",
      "\n",
      "        [[-3.0000],\n",
      "         [-3.0000],\n",
      "         [-3.0000],\n",
      "         [-3.0000]],\n",
      "\n",
      "        [[ 6.0100],\n",
      "         [ 6.1600],\n",
      "         [ 5.8000],\n",
      "         [ 2.2000]],\n",
      "\n",
      "        [[-3.0000],\n",
      "         [-3.0000],\n",
      "         [-3.0000],\n",
      "         [-3.0000]]]),)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients = torch.autograd.grad(y, x, grad_outputs= grad_mask, is_grads_batched=True, retain_graph=True)\n",
    "\n",
    "# Autograd: Compute and return the sum of gradients of outputs with respect to the inputs.\n",
    "print(\"\\nGradients - Autograd: \\n\", gradients) # grad_outputs should be a sequence of length matching output containing the “vector” in vector-Jacobian product (v^T@J)\n",
    "gradients[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0000, -3.0000,  6.0100, -3.0000],\n",
       "        [ 3.0000, -3.0000,  6.1600, -3.0000],\n",
       "        [ 3.0000, -3.0000,  5.8000, -3.0000],\n",
       "        [ 3.0000, -3.0000,  2.2000, -3.0000]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.func.jvp(y_fun, (x,), (torch.ones_like(x),))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set the default automatic differentiation to forward mode.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m deepxde\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mset_default_autodiff(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#gradients_dxde = deepxde.gradients.gradients.jacobian(y_fun(x), x, j = 0)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m gradients_dxde \u001b[38;5;241m=\u001b[39m \u001b[43mdeepxde\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Autograd: Compute and return the sum of gradients of outputs with respect to the inputs.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGradients - Autograd: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, gradients_dxde) \u001b[38;5;66;03m# grad_outputs should be a sequence of length matching output containing the “vector” in vector-Jacobian product (v^T@J)\u001b[39;00m\n",
      "File \u001b[0;32m/leonardo_scratch/fast/IscrC_DL4EO/my_venv_fast/lib/python3.11/site-packages/deepxde/gradients/gradients.py:63\u001b[0m, in \u001b[0;36mhessian\u001b[0;34m(ys, xs, component, i, j)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gradients_reverse\u001b[38;5;241m.\u001b[39mhessian(ys, xs, component\u001b[38;5;241m=\u001b[39mcomponent, i\u001b[38;5;241m=\u001b[39mi, j\u001b[38;5;241m=\u001b[39mj)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mautodiff \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgradients_forward\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/leonardo_scratch/fast/IscrC_DL4EO/my_venv_fast/lib/python3.11/site-packages/deepxde/gradients/gradients_forward.py:111\u001b[0m, in \u001b[0;36mhessian\u001b[0;34m(ys, xs, component, i, j)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhessian\u001b[39m(ys, xs, component\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, j\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 111\u001b[0m     dys_xj \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jacobian(dys_xj, xs, i\u001b[38;5;241m=\u001b[39mcomponent, j\u001b[38;5;241m=\u001b[39mi)\n",
      "File \u001b[0;32m/leonardo_scratch/fast/IscrC_DL4EO/my_venv_fast/lib/python3.11/site-packages/deepxde/gradients/gradients_forward.py:104\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(ys, xs, i, j)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjacobian\u001b[39m(ys, xs, i\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, j\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjacobian\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Jacobians\u001b[49m\u001b[43m(\u001b[49m\u001b[43mys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/leonardo_scratch/fast/IscrC_DL4EO/my_venv_fast/lib/python3.11/site-packages/deepxde/gradients/jacobian.py:128\u001b[0m, in \u001b[0;36mJacobians.__call__\u001b[0;34m(self, ys, xs, i, j)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJs:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJs[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJacobianClass(ys, xs)\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/leonardo_scratch/fast/IscrC_DL4EO/my_venv_fast/lib/python3.11/site-packages/deepxde/gradients/gradients_forward.py:64\u001b[0m, in \u001b[0;36mJacobianForward.__call__\u001b[0;34m(self, i, j)\u001b[0m\n\u001b[1;32m     61\u001b[0m     grad_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39mjvp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mys[\u001b[38;5;241m1\u001b[39m], (x,), (tangent,))[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# jvp by torch.autograd.functional.jvp\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# grad_fn = lambda x: torch.autograd.functional.jvp(self.ys[1], (x,), (tangent,), create_graph=True)[1]\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJ[j] \u001b[38;5;241m=\u001b[39m (\u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m, grad_fn)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Here, we use jax.jvp to compute the gradient of a function. This is\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# different from TensorFlow and PyTorch that the input of a function is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Other option is jax.jacfwd + jax.vmap, which could be used to compute\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# the full Jacobian matrix efficiently, if needed.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     tangent \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_x)\u001b[38;5;241m.\u001b[39mat[j]\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/leonardo_scratch/fast/IscrC_DL4EO/my_venv_fast/lib/python3.11/site-packages/deepxde/gradients/gradients_forward.py:61\u001b[0m, in \u001b[0;36mJacobianForward.__call__.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     59\u001b[0m tangent \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxs)\n\u001b[1;32m     60\u001b[0m tangent[:, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 61\u001b[0m grad_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39mjvp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, (x,), (tangent,))[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# jvp by torch.autograd.functional.jvp\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# grad_fn = lambda x: torch.autograd.functional.jvp(self.ys[1], (x,), (tangent,), create_graph=True)[1]\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJ[j] \u001b[38;5;241m=\u001b[39m (grad_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxs), grad_fn)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "deepxde.config.set_default_autodiff(\"forward\")\n",
    "#gradients_dxde = deepxde.gradients.gradients.jacobian(y_fun(x), x, j = 0)\n",
    "\n",
    "gradients_dxde = deepxde.gradients.gradients.hessian((y,), x, component=0 , j=0)\n",
    "# Autograd: Compute and return the sum of gradients of outputs with respect to the inputs.\n",
    "print(\"\\nGradients - Autograd: \\n\", gradients_dxde) # grad_outputs should be a sequence of length matching output containing the “vector” in vector-Jacobian product (v^T@J)\n",
    "gradients_dxde[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-sample-grads autograd <torch.utils.benchmark.utils.common.Measurement object at 0x7fab48643990>\n",
      "torch.autograd.grad(y, x, grad_outputs= grad_mask, is_grads_batched=True, retain_graph=True)\n",
      "  629.52 us\n",
      "  1 measurement, 50 runs , 1 thread\n",
      "Per-sample-grads forward_ad <torch.utils.benchmark.utils.common.Measurement object at 0x7fab48624f50>\n",
      "third_res = torch.func.jvp(y_fun, (x,), (torch.ones_like(x),))\n",
      "  1.81 ms\n",
      "  1 measurement, 50 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark import Timer\n",
    "\n",
    "#backward_loop = Timer(stmt=\"grad_bk_loop(y, x)\", globals=globals())\n",
    "autograd = Timer(stmt=\"torch.autograd.grad(y, x, grad_outputs= grad_mask, is_grads_batched=True, retain_graph=True)\",globals=globals())\n",
    "forward_ad = Timer(stmt=\"third_res = torch.func.jvp(y_fun, (x,), (torch.ones_like(x),))\", globals=globals())\n",
    "#backward_loop_timing = backward_loop.timeit(50)\n",
    "autograd_timing = autograd.timeit(50)\n",
    "forward_ad_timing = forward_ad.timeit(50)\n",
    "\n",
    "#print(f'Per-sample-grads backward_loop {backward_loop_timing}')\n",
    "print(f'Per-sample-grads autograd {autograd_timing}')\n",
    "print(f'Per-sample-grads forward_ad {forward_ad_timing}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-sample-grads backward_loop <torch.utils.benchmark.utils.common.Measurement object at 0x7f6692b42c90>\n",
      "grad_bk_loop(y, x)\n",
      "  5.75 ms\n",
      "  1 measurement, 50 runs , 1 thread\n",
      "Per-sample-grads autograd <torch.utils.benchmark.utils.common.Measurement object at 0x7f6692ab00d0>\n",
      "torch.autograd.grad(y, x, grad_outputs= grad_mask, is_grads_batched=True, retain_graph=True)\n",
      "  627.74 us\n",
      "  1 measurement, 50 runs , 1 thread\n",
      "Performance delta: 8.1520 percent improvement with autograd \n"
     ]
    }
   ],
   "source": [
    "def get_perf(first, first_descriptor, second, second_descriptor):\n",
    "    \"\"\"takes torch.benchmark objects and compares delta of second vs first.\"\"\"\n",
    "    second_res = second.times[0]\n",
    "    first_res = first.times[0]\n",
    "\n",
    "    gain = (first_res-second_res)/first_res\n",
    "    if gain < 0: gain *=-1\n",
    "    final_gain = gain\n",
    "\n",
    "    print(f\"Performance delta: {final_gain:.4f} percent improvement with {first_descriptor} \")\n",
    "\n",
    "from torch.utils.benchmark import Timer\n",
    "\n",
    "backward_loop = Timer(stmt=\"grad_bk_loop(y, x)\", globals=globals())\n",
    "autograd = Timer(stmt=\"torch.autograd.grad(y, x, grad_outputs= grad_mask, is_grads_batched=True, retain_graph=True)\",globals=globals())\n",
    "backward_loop_timing = backward_loop.timeit(50)\n",
    "autograd_timing = autograd.timeit(50)\n",
    "\n",
    "print(f'Per-sample-grads backward_loop {backward_loop_timing}')\n",
    "print(f'Per-sample-grads autograd {autograd_timing}')\n",
    "\n",
    "get_perf(autograd_timing, \"autograd\", backward_loop_timing, \"backprop loop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv_fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
